{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LGGEcGxn509wY-03C8k5eyYmNfXdRqOG",
      "authorship_tag": "ABX9TyPxcoyWJCLiJI5JzJuSt55d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gilgamesh60/AnimeCharactersFaceRecognition-unfinished-/blob/main/Model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X-8qwaAZZ7V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIm8Sy1DRXnS"
      },
      "outputs": [],
      "source": [
        "path_test = '/content/drive/MyDrive/cropped_images'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2"
      ],
      "metadata": {
        "id": "a2xbwIP6SG4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORIES=['ayanokoji_kiyotaka','rikka_takanashi','nami','emilia_re-zero','rimuru','senjogahara_hitagi','miku_nakano','lelouch_lamperouge','hayasaka_ai','marin_kitagawa']"
      ],
      "metadata": {
        "id": "5cZZ6GxQRp55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict={}\n",
        "count=0\n",
        "for category in CATEGORIES:\n",
        "  class_dict[category]=count\n",
        "  count+=1\n",
        "class_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LC713c2jt3Jb",
        "outputId": "5d878925-22e5-4d24-f14d-9875353b325a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ayanokoji_kiyotaka': 0,\n",
              " 'rikka_takanashi': 1,\n",
              " 'nami': 2,\n",
              " 'emilia_re-zero': 3,\n",
              " 'rimuru': 4,\n",
              " 'senjogahara_hitagi': 5,\n",
              " 'miku_nakano': 6,\n",
              " 'lelouch_lamperouge': 7,\n",
              " 'hayasaka_ai': 8,\n",
              " 'marin_kitagawa': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "y=[]\n",
        "IMG_SIZE =227\n",
        "def createTrainingData():\n",
        "  for category in CATEGORIES:\n",
        "    path = os.path.join(path_test, category)\n",
        "    class_num = CATEGORIES.index(category)\n",
        "    for img in os.listdir(path):\n",
        "      img_array = cv2.imread(os.path.join(path,img))\n",
        "      if img_array is None:\n",
        "        continue\n",
        "      new=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
        "      coloured=cv2.cvtColor(new,cv2.COLOR_BGR2RGB)\n",
        "      images.append(coloured)\n",
        "      y.append(class_dict[category])"
      ],
      "metadata": {
        "id": "p-bHqo-4SjjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "createTrainingData()"
      ],
      "metadata": {
        "id": "gyOQyNYyqX6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X =[]\n",
        "y =[]\n",
        "for features, label in images:\n",
        "  X.append(features)\n",
        "  y.append(label)"
      ],
      "metadata": {
        "id": "HdvYHQJwUk9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 200\n",
        "\n",
        "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "SA_PXSoAcaIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root_dir='/content/drive/MyDrive/cropped_images/'\n",
        "os.makedirs(root_dir+'train/')\n",
        "os.makedirs(root_dir+'test/')"
      ],
      "metadata": {
        "id": "ZcrMZnAHArb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shutil\n",
        "import random\n",
        "test_ratio=0.2\n",
        "CATEGORIES=['ayanokoji_kiyotaka','rikka_takanashi','nami','emilia_re-zero','rimuru','senjogahara_hitagi','miku_nakano','lelouch_lamperouge','hayasaka_ai','marin_kitagawa']\n",
        "for category in CATEGORIES:\n",
        "  os.makedirs(root_dir+'train/'+category)\n",
        "  os.makedirs(root_dir+'test/'+category)\n",
        "  train_files=[]\n",
        "  test_files=[]\n",
        "  src=root_dir+category\n",
        "  files=os.listdir(src)\n",
        "  np.random.shuffle(files)\n",
        "  train_images, test_images = np.split(np.array(files),[int(len(files)* (1 - test_ratio))])\n",
        "  train_files=[src+'/'+name for name in train_images]\n",
        "  test_files=[src+'/'+name for name in train_images]\n",
        "  for file_name in train_files:\n",
        "    shutil.copy(file_name,root_dir+'train/'+category)\n",
        "  for file_name in test_files:\n",
        "    shutil.copy(file_name,root_dir+'test/'+category)"
      ],
      "metadata": {
        "id": "J5UNROQL53gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer=transforms.Compose([transforms.Resize(32),transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "5qqNlzXptBmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=datasets.ImageFolder('/content/drive/MyDrive/cropped_images/train',transform=transformer)\n",
        "test_dataset=datasets.ImageFolder('/content/drive/MyDrive/cropped_images/test',transform=transformer)"
      ],
      "metadata": {
        "id": "f5RLTPTksGmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set=torch.utils.data.DataLoader(train_dataset,shuffle=True,batch_size=4)\n",
        "test_set=torch.utils.data.DataLoader(test_dataset,shuffle=True,batch_size=4)"
      ],
      "metadata": {
        "id": "v7zGotsAR7uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels= 96, kernel_size=7, stride=2,padding=2)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride= 1, padding= 2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride= 1, padding= 1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1  = nn.Linear(in_features= 256*3*3, out_features= 1024)\n",
        "        self.fc2  = nn.Linear(in_features= 1024, out_features= 512)\n",
        "        self.fc3 = nn.Linear(in_features=512 , out_features=10)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Dr9m0KgVvpHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AlexNet()\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXKons7HS3bc",
        "outputId": "99e5af57-3a3f-469d-86d0-711ea54b1e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (conv1): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(2, 2))\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (conv3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv5): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=2304, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc3): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.container import ModuleList\n",
        "from torch import optim\n",
        "#training the model\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=AlexNet()\n",
        "model=model.to(device=device)\n",
        "\n",
        "learning_rate=1e-4\n",
        "load_model=True\n",
        "epochs=17\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer =optim.Adam(params= model.parameters(),lr=learning_rate)\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    for image,label in train_set:\n",
        "       image=image.to(device)\n",
        "       label=label.to(device)\n",
        "       output = model(image)\n",
        "       loss = loss_function(output,label)\n",
        "\n",
        "       optimizer.zero_grad()\n",
        "        \n",
        "       loss.backward()\n",
        "\n",
        "       optimizer.step()\n",
        "    \n",
        "    print(f'Epoch {epoch+1}/{epochs}: Training Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mb9mKY8yEtP",
        "outputId": "4dbe5bff-7b41-4138-d665-eb0a8fecc321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/17: Training Loss: 2.2955541610717773\n",
            "Epoch 2/17: Training Loss: 2.9832160472869873\n",
            "Epoch 3/17: Training Loss: 1.4030698537826538\n",
            "Epoch 4/17: Training Loss: 3.3637428283691406\n",
            "Epoch 5/17: Training Loss: 1.1573679447174072\n",
            "Epoch 6/17: Training Loss: 0.7340166568756104\n",
            "Epoch 7/17: Training Loss: 0.22163547575473785\n",
            "Epoch 8/17: Training Loss: 0.10277438163757324\n",
            "Epoch 9/17: Training Loss: 0.0014359832275658846\n",
            "Epoch 10/17: Training Loss: 0.5269375443458557\n",
            "Epoch 11/17: Training Loss: 0.017077535390853882\n",
            "Epoch 12/17: Training Loss: 0.5937560796737671\n",
            "Epoch 13/17: Training Loss: 0.0019223210401833057\n",
            "Epoch 14/17: Training Loss: 0.020727848634123802\n",
            "Epoch 15/17: Training Loss: 0.005889357533305883\n",
            "Epoch 16/17: Training Loss: 0.19996114075183868\n",
            "Epoch 17/17: Training Loss: 0.0007490348070859909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct=0\n",
        "incorrect=0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for image,label in test_set:\n",
        "      image = image.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      output = model(image)\n",
        "      temp,pred=torch.max(output,1)\n",
        "      correct+= (pred==label).sum().item()\n",
        "      incorrect+= (pred!=label).sum().item()\n",
        "\n",
        "      model_accuracy =(correct/(correct+incorrect))*100\n",
        "  \n",
        "  \n",
        "  \n",
        "print(\"The model accuracy is :\",model_accuracy,\"%\",sep=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLQpboG0YLHw",
        "outputId": "f81b154a-44e9-42dd-ca6e-e1e1b3574ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model accuracy is : 96.86520376175548 %\n"
          ]
        }
      ]
    }
  ]
}